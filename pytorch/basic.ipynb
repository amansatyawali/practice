{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We move our tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "  tensor = tensor.to('cuda')\n",
    "  print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=0)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5380, 0.9776],\n",
      "        [0.3663, 0.3875]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(2, 2)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0760, 1.9551],\n",
      "        [0.7326, 0.7751]])\n",
      "tensor([[0.2895, 0.9556],\n",
      "        [0.1342, 0.1502]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor + tensor)\n",
    "print(tensor * tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[356.0289, 259.4393, 259.9976,  ..., 268.1339, 261.7565, 270.4874],\n",
       "        [259.4393, 331.5832, 247.6438,  ..., 254.6568, 253.1635, 258.3434],\n",
       "        [259.9976, 247.6438, 334.7607,  ..., 257.5806, 253.9530, 255.6121],\n",
       "        ...,\n",
       "        [268.1339, 254.6568, 257.5805,  ..., 342.2781, 264.5026, 262.0823],\n",
       "        [261.7565, 253.1636, 253.9530,  ..., 264.5026, 347.5656, 261.9825],\n",
       "        [270.4874, 258.3434, 255.6121,  ..., 262.0823, 261.9825, 348.3051]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.rand(1000, 1000) \n",
    "tensor.matmul(tensor.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.4484, 5.8294, 5.8123, 5.7519],\n",
      "        [5.6575, 5.3059, 5.5248, 5.9924],\n",
      "        [5.7652, 5.5874, 5.5586, 5.3188]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "labels = torch.rand(1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.8451, -4.9175, -4.7626, -5.7902, -5.2996, -4.6637, -4.7945, -4.0148,\n",
       "         -4.0001, -5.8622, -5.3986, -5.1339, -4.7566, -5.5330, -5.5971, -4.7801,\n",
       "         -5.0582, -4.7883, -5.0481, -4.9423, -5.8758, -4.9866, -5.7600, -4.2509,\n",
       "         -5.1759, -5.5144, -5.3117, -5.4980, -5.3330, -4.9205, -5.0453, -5.3611,\n",
       "         -4.8221, -4.9153, -4.8301, -4.9271, -3.7658, -5.0371, -5.0968, -4.4300,\n",
       "         -5.2577, -5.3010, -5.5412, -4.7896, -5.1432, -4.6451, -5.4196, -5.3022,\n",
       "         -5.2977, -5.9018, -4.7799, -3.9067, -4.8257, -4.9389, -4.4617, -5.7653,\n",
       "         -4.5899, -5.8080, -4.8183, -5.0241, -3.8475, -4.3281, -4.6338, -4.4357,\n",
       "         -5.5714, -4.7774, -4.9386, -4.7157, -5.2462, -5.6890, -6.2341, -4.3597,\n",
       "         -6.0955, -5.2096, -5.7599, -5.7676, -4.4058, -5.2988, -4.4317, -4.7542,\n",
       "         -5.4041, -5.7202, -4.1892, -5.0231, -5.2623, -4.4428, -4.0437, -4.0106,\n",
       "         -4.4727, -5.2436, -5.5400, -5.5595, -6.1655, -4.7867, -4.4694, -6.6098,\n",
       "         -5.1571, -4.6249, -5.9915, -4.6764, -5.8076, -5.2133, -5.2502, -4.8119,\n",
       "         -4.9946, -4.9297, -5.0500, -5.7698, -5.4379, -5.8076, -5.6242, -5.0971,\n",
       "         -3.4290, -4.2928, -4.5264, -5.7487, -5.1327, -4.7518, -3.8393, -4.7522,\n",
       "         -5.2352, -4.4324, -4.2802, -4.2103, -3.6257, -4.4116, -4.2845, -5.9441,\n",
       "         -5.9337, -5.5544, -6.1500, -5.8022, -5.6052, -5.5888, -5.3365, -5.6278,\n",
       "         -5.6043, -5.7543, -5.8087, -5.9946, -6.1497, -6.2061, -6.4484, -6.1079,\n",
       "         -5.3340, -4.7319, -5.4730, -6.2908, -5.8341, -5.7646, -3.9551, -3.1264,\n",
       "         -5.4245, -5.0275, -4.1139, -4.2562, -4.5983, -4.5950, -4.5531, -4.4227,\n",
       "         -3.8221, -3.8377, -4.3926, -3.9413, -3.9111, -4.6338, -4.8758, -5.2330,\n",
       "         -4.2011, -4.9586, -4.6681, -3.8679, -4.1931, -4.4729, -4.3383, -5.2968,\n",
       "         -4.4658, -4.8066, -3.7460, -3.8439, -3.8684, -4.5097, -4.0625, -4.3023,\n",
       "         -3.9301, -3.9051, -4.1035, -4.5034, -4.6971, -4.2056, -5.1760, -4.2674,\n",
       "         -4.2083, -4.0629, -5.3300, -3.7953, -4.4333, -4.4452, -4.5207, -4.1028,\n",
       "         -4.2071, -4.3978, -3.8652, -4.1689, -4.2118, -4.0673, -4.4306, -3.6280,\n",
       "         -3.2119, -3.9064, -4.4086, -4.1661, -3.6931, -4.3984, -4.2282, -4.0947,\n",
       "         -4.1460, -3.8782, -4.7363, -3.4114, -3.8920, -4.4480, -4.3331, -3.7560,\n",
       "         -4.2280, -4.1148, -4.3392, -3.7211, -4.7875, -4.8291, -4.5236, -3.8677,\n",
       "         -4.0273, -4.6563, -3.5446, -3.8132, -4.2498, -4.0948, -3.5876, -4.3790,\n",
       "         -3.8171, -4.3352, -3.9559, -4.0636, -4.7079, -4.0776, -4.1982, -4.1761,\n",
       "         -3.8360, -4.2055, -4.0450, -3.7605, -5.3992, -4.0236, -3.6056, -4.8582,\n",
       "         -3.8438, -3.8753, -4.4051, -3.9592, -4.5323, -4.8804, -4.9804, -4.1516,\n",
       "         -3.8402, -3.7116, -4.2116, -3.6842, -4.9189, -5.0390, -5.3736, -5.5481,\n",
       "         -5.2221, -4.0764, -5.9449, -5.5438, -5.6531, -5.4752, -5.7851, -5.0562,\n",
       "         -4.8634, -3.8098, -3.6375, -4.4381, -4.0477, -3.8626, -4.5806, -4.7426,\n",
       "         -5.1313, -6.0340, -5.3588, -5.4676, -4.6525, -5.4468, -5.4104, -5.3298,\n",
       "         -5.2978, -5.6348, -4.8302, -4.4431, -6.2501, -5.2517, -4.9623, -4.9928,\n",
       "         -5.8757, -5.4063, -4.4048, -5.4835, -5.5918, -5.0030, -4.2858, -5.0634,\n",
       "         -5.3147, -4.5453, -4.0080, -5.0950, -5.5909, -5.6921, -5.8430, -5.4660,\n",
       "         -6.1323, -5.6505, -6.0128, -5.7543, -6.0182, -6.0359, -5.8011, -4.1300,\n",
       "         -4.4397, -5.0206, -4.8272, -4.9588, -4.4866, -4.3895, -5.0184, -5.6838,\n",
       "         -5.7250, -4.2689, -3.7547, -5.7048, -4.9217, -4.1474, -5.0512, -5.6577,\n",
       "         -5.1490, -3.8114, -5.1240, -5.9492, -4.4157, -5.7320, -6.0130, -6.7011,\n",
       "         -6.0780, -5.5399, -5.2368, -4.2256, -3.5352, -4.6708, -4.1507, -4.0904,\n",
       "         -4.7272, -3.9441, -4.1789, -4.2550, -4.7301, -4.8017, -5.3550, -4.8633,\n",
       "         -5.1007, -5.1777, -5.0678, -4.5873, -4.7145, -4.5385, -4.7876, -5.5198,\n",
       "         -5.3146, -4.3298, -5.0129, -5.0272, -4.5766, -5.0051, -4.7821, -5.0668,\n",
       "         -5.2955, -4.9630, -5.3301, -5.6003, -5.2444, -4.6741, -3.8538, -4.2370,\n",
       "         -5.9821, -6.0150, -4.6498, -4.1640, -5.6575, -5.1589, -4.0628, -4.3625,\n",
       "         -5.4835, -3.7881, -4.2719, -6.6058, -6.1363, -4.9184, -4.7763, -4.8311,\n",
       "         -4.8502, -3.3934, -4.6849, -4.4602, -2.6333, -3.8781, -4.0509, -3.6407,\n",
       "         -4.9473, -4.5033, -4.4489, -3.6958, -3.5228, -3.4025, -4.6201, -4.2282,\n",
       "         -4.4184, -5.3707, -4.7367, -3.2611, -2.6315, -4.1619, -5.3242, -4.4677,\n",
       "         -4.4168, -4.0185, -4.1923, -3.3598, -4.7867, -5.0647, -4.2354, -4.6940,\n",
       "         -3.7059, -3.9466, -4.7158, -5.3068, -4.6482, -4.3116, -3.8951, -2.3608,\n",
       "         -3.7318, -5.1171, -4.9182, -4.0189, -4.1717, -4.6105, -4.6889, -3.8771,\n",
       "         -3.5670, -3.5198, -4.4311, -4.1460, -5.1809, -4.2964, -3.3043, -2.2533,\n",
       "         -3.1842, -5.0463, -5.7136, -4.6974, -4.3723, -3.2590, -3.4519, -3.7786,\n",
       "         -4.1448, -3.5308, -5.0073, -4.2841, -4.5237, -3.9179, -4.0747, -3.9160,\n",
       "         -4.3779, -4.2078, -3.9558, -5.5586, -5.9971, -4.2321, -4.7807, -3.1955,\n",
       "         -2.7146, -3.4168, -3.8020, -3.7110, -3.7613, -5.6234, -3.4150, -5.4280,\n",
       "         -4.5627, -5.0626, -4.6129, -3.6757, -6.2620, -4.0326, -3.0590, -4.2019,\n",
       "         -3.7692, -3.6487, -3.7500, -3.8307, -3.8750, -4.2965, -5.9214, -5.0664,\n",
       "         -3.4804, -4.0655, -3.2814, -2.7186, -4.2019, -4.4684, -3.3872, -4.0426,\n",
       "         -5.4170, -4.2983, -3.6143, -2.5893, -4.5219, -5.1192, -4.5202, -4.7307,\n",
       "         -3.8295, -4.8632, -3.3940, -4.2279, -4.3902, -5.2692, -4.0476, -4.9044,\n",
       "         -5.2003, -4.9911, -4.5156, -3.2183, -5.7638, -2.7180, -3.4596, -3.6408,\n",
       "         -4.0718, -3.5725, -3.8935, -6.6348, -5.6836, -4.5935, -5.0356, -4.2258,\n",
       "         -3.9234, -4.1951, -6.2930, -5.3081, -4.2686, -3.9875, -3.3044, -3.3732,\n",
       "         -4.6099, -4.7111, -3.8227, -4.4451, -6.0480, -5.2136, -3.9184, -3.3392,\n",
       "         -4.2356, -5.0301, -3.3073, -4.2074, -3.3055, -4.8870, -3.8154, -4.3885,\n",
       "         -5.4774, -3.5856, -4.0842, -4.7350, -4.1980, -4.6482, -3.8248, -4.1420,\n",
       "         -3.7216, -3.7828, -4.7970, -2.9624, -3.6308, -3.0269, -4.9500, -4.2334,\n",
       "         -5.1766, -3.7010, -4.5908, -4.9724, -3.5845, -4.7577, -5.0099, -3.3425,\n",
       "         -2.0918, -4.8447, -4.8923, -4.7950, -4.1206, -4.4401, -3.1089, -4.7633,\n",
       "         -4.2637, -4.5147, -3.8672, -3.8560, -4.7956, -3.7988, -4.5643, -4.4138,\n",
       "         -3.5180, -4.3854, -2.6436, -3.3828, -3.5207, -3.4115, -4.3437, -4.0158,\n",
       "         -4.2017, -5.6496, -3.5200, -4.7422, -5.7616, -4.0676, -4.4810, -3.4425,\n",
       "         -3.7493, -3.3829, -4.9036, -4.4380, -3.3586, -3.8033, -4.0161, -4.2884,\n",
       "         -5.9587, -3.3471, -4.7071, -3.2529, -3.9252, -5.5147, -4.0823, -4.3636,\n",
       "         -4.9091, -5.7681, -3.0880, -4.2205, -3.9352, -3.6132, -4.5573, -3.9949,\n",
       "         -4.5107, -4.3581, -4.3834, -4.0806, -4.5985, -5.3255, -4.5290, -5.6797,\n",
       "         -3.4760, -4.1473, -3.4692, -4.0786, -5.5687, -5.0935, -3.9695, -4.5526,\n",
       "         -4.8190, -3.7208, -3.4993, -5.2748, -2.9439, -3.5526, -3.7568, -4.2665,\n",
       "         -3.9235, -3.7976, -5.2045, -4.0294, -3.7230, -5.9912, -4.5552, -5.6426,\n",
       "         -4.8380, -5.6233, -5.0647, -3.7267, -3.8679, -3.9008, -5.4927, -3.3880,\n",
       "         -2.8916, -4.4831, -5.0923, -3.8911, -2.9755, -5.1128, -4.9581, -4.0830,\n",
       "         -3.9239, -4.8211, -4.8249, -4.0698, -3.7152, -4.3739, -3.2496, -3.2196,\n",
       "         -4.4428, -4.7931, -4.0050, -5.2694, -3.6439, -4.7456, -4.9618, -3.7855,\n",
       "         -4.2305, -4.3662, -3.5055, -4.1954, -5.4435, -3.2450, -5.3414, -4.9014,\n",
       "         -3.3227, -4.8498, -4.1976, -2.4839, -5.0621, -2.8053, -6.2363, -4.3229,\n",
       "         -4.7082, -3.8789, -3.7966, -4.2235, -3.3430, -4.7676, -3.8761, -4.3075,\n",
       "         -3.7532, -4.1804, -4.4430, -4.0302, -3.9970, -3.4911, -4.2366, -4.6834,\n",
       "         -4.7193, -3.8409, -3.5005, -4.9151, -3.4756, -4.5401, -3.0091, -4.6622,\n",
       "         -4.1533, -3.6694, -4.0207, -4.0182, -3.3887, -3.5517, -4.0250, -3.9686,\n",
       "         -4.6863, -3.5203, -3.9648, -3.8435, -2.9857, -3.5841, -3.6217, -3.9327,\n",
       "         -4.0606, -4.0285, -3.1600, -5.0094, -5.5239, -5.4789, -3.3203, -3.6160,\n",
       "         -2.8668, -4.1382, -3.8196, -3.3348, -3.9030, -4.8776, -4.1219, -3.2795,\n",
       "         -2.7593, -3.5708, -4.3499, -4.6496, -3.5740, -4.0319, -5.4718, -4.1991,\n",
       "         -5.1879, -4.3650, -5.2743, -5.6470, -3.4001, -3.3133, -4.1474, -4.5159,\n",
       "         -3.1933, -4.5123, -4.9424, -3.2339, -4.7924, -2.8410, -5.5577, -4.8844,\n",
       "         -4.1481, -5.6420, -2.4809, -3.8595, -5.9152, -5.7830, -3.8976, -3.8619,\n",
       "         -3.5422, -5.1483, -4.1115, -3.3442, -3.0920, -5.1781, -3.2546, -4.0216,\n",
       "         -5.4302, -5.4070, -4.5924, -4.1278, -2.6813, -2.7608, -3.3574, -5.4315,\n",
       "         -2.8856, -4.3244, -4.1925, -4.3146, -3.6723, -2.9960, -3.3322, -4.9415,\n",
       "         -3.8597, -3.9973, -3.2376, -2.9186, -2.8430, -5.0582, -5.0822, -3.4212,\n",
       "         -4.9063, -4.7827, -4.7351, -3.2597, -4.4709, -3.0321, -4.2578, -4.8520,\n",
       "         -5.0224, -4.0274, -4.1476, -4.7423, -3.0696, -5.0154, -3.0926, -6.1284,\n",
       "         -3.3480, -5.8937, -7.0568, -4.1740, -3.0345, -4.6282, -4.7746, -3.1160,\n",
       "         -3.1303, -4.8155, -3.3840, -3.1233, -4.5510, -4.5249, -4.8344, -4.9298,\n",
       "         -5.7650, -4.4893, -5.1313, -3.8098, -3.4828, -4.4406, -5.0008, -5.2371,\n",
       "         -3.2841, -4.0331, -2.5854, -2.6114, -5.5074, -5.0391, -2.8346, -3.7725,\n",
       "         -3.6469, -4.4000, -5.3153, -3.1680, -5.3612, -3.4574, -3.5186, -3.5578,\n",
       "         -3.9756, -5.1102, -6.7055, -5.0843, -4.2550, -4.2227, -4.0138, -4.4123,\n",
       "         -4.6950, -3.4298, -5.2192, -3.7310, -4.9073, -5.4137, -5.5492, -4.9738,\n",
       "         -4.6841, -3.2229, -4.6871, -4.5105, -3.9747, -6.3203, -4.4508, -4.8432,\n",
       "         -4.2493, -4.3627, -4.4249, -4.7321, -4.2939, -5.0239, -4.5339, -4.2205,\n",
       "         -4.9568, -5.2371, -5.5985, -3.9296, -4.0864, -4.8862, -4.3981, -4.9823,\n",
       "         -5.0621, -4.2550, -3.6110, -4.8972, -4.7413, -4.8698, -4.3260, -5.2308,\n",
       "         -4.2293, -4.1013, -4.8675, -5.0679, -5.8506, -4.8328, -4.0981, -5.1959,\n",
       "         -3.6857, -4.4082, -4.5625, -3.6906, -4.6263, -4.5508, -6.7505, -3.5387,\n",
       "         -5.9673, -4.0878, -4.2276, -5.2679, -5.0608, -4.7257, -4.0365, -4.8239,\n",
       "         -5.1113, -5.8182, -7.0528, -3.1772, -4.7216, -5.5483, -4.9495, -5.5104,\n",
       "         -5.2436, -6.0049, -4.9813, -4.7423, -4.1655, -4.8524, -3.3426, -3.6761]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model(data) # forward pass\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7175243730c9031ba60f56d772a877afc89cdaf535e418218038fcc676c9afa7"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
