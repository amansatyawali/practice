{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1232754c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk.wordnet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-1ac1458dc32d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnowball\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk.wordnet'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f9ba3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /home/aman.satyawali/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf375061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e07df9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "whitman_leaves_words = nltk.corpus.gutenberg.words('whitman-leaves.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e92576d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154883"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(whitman_leaves_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f95284",
   "metadata": {},
   "source": [
    "<h3>Removing special characters</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2d139f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_regex(input_text, regex_pattern) :\n",
    "    urls = re.finditer(regex_pattern, input_text)\n",
    "    for i in urls :\n",
    "        input_text = re.sub(i.group(), '', input_text)\n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c7c2c570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove this  from  given string\n"
     ]
    }
   ],
   "source": [
    "ptrn = '#[\\w]*'\n",
    "\n",
    "output = remove_regex('remove this #hashtag from #my given string', ptrn)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260a9603",
   "metadata": {},
   "source": [
    "<h3>Removing white spaces</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "358e0137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t this is the sample string \n",
      "this is the sample string\n"
     ]
    }
   ],
   "source": [
    "inp = '\\t this is the sample string '\n",
    "print(inp)\n",
    "output = inp.strip()\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e8d21739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'the', 'sample', 'string']\n"
     ]
    }
   ],
   "source": [
    "print(output.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fbd51c",
   "metadata": {},
   "source": [
    "<h3>Removing numbers</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8dc80b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(input_text) :\n",
    "    ptrn = '\\s*[0-9]+'\n",
    "    \n",
    "    urls = re.finditer(ptrn, input_text)\n",
    "    \n",
    "    for i in urls :\n",
    "        print(i.group())\n",
    "        input_text = re.sub(i.group(), '', input_text)\n",
    "    \n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "91e2a283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 123\n",
      " 2\n",
      "2\n",
      "Hello,here, I am\n"
     ]
    }
   ],
   "source": [
    "input_text = 'Hello 123, 2here, I a2m'\n",
    "\n",
    "output = remove_numbers(input_text)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0891ac",
   "metadata": {},
   "source": [
    "<h3>Converting to lowercase</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4eab9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e32884d7",
   "metadata": {},
   "source": [
    "<h3>Tokenization</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cee8106c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/aman.satyawali/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40ffb6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5026369a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ff948869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Natural', 'Language', 'Toolkit', ',', 'or', 'more', 'commonly', 'NLTK', ',', 'is', 'a', 'suite', 'of', 'libraries', 'and', 'programs', 'for', 'symbolic', 'and', 'statistical', 'natural', 'language', 'processing', 'for', 'English', 'written', 'in', 'the', 'Python', 'programming', 'language', '.']\n"
     ]
    }
   ],
   "source": [
    "input_text = 'The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing for English written in the Python programming language.'\n",
    "tokens = word_tokenize(input_text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb6a264",
   "metadata": {},
   "source": [
    "<h3>Removing stop words</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5aac2116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/aman.satyawali/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8cb294d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'when', 'shouldn', 'hadn', 'do', 'mustn', 'why', 'did', 'an', 'it', 'that', 'so', 'further', \"couldn't\", 'which', 'above', 'd', 'weren', 's', 'won', \"you'd\", \"wouldn't\", 'no', 'herself', 'if', 'to', 'his', 'will', 'most', 'me', 'or', \"shouldn't\", 'than', 'o', 'ma', 'between', 'there', \"doesn't\", 'other', \"should've\", 'itself', \"won't\", 'here', 'hers', 'she', 'a', 'needn', 'had', 'yourselves', 'm', 'ours', 'you', 'too', 'doesn', 'these', 'my', 'into', 'didn', 'been', 'again', \"wasn't\", 'shan', 'in', 'hasn', 'her', \"she's\", 'was', \"mightn't\", 'wasn', \"mustn't\", 'am', 'more', 'he', 'for', 'not', \"shan't\", 'are', 'out', 'aren', \"don't\", 'should', 'after', \"you're\", 'over', \"needn't\", 't', 'him', 'mightn', \"it's\", 'doing', 'how', 'now', 'up', 'each', 'all', 'their', 'isn', 'being', 'what', \"you'll\", 'own', 'were', 'himself', 'have', 'they', 'who', 'by', \"hadn't\", 'before', 'whom', 'does', 'll', 'through', 'where', 'very', 'nor', 'its', 'under', \"weren't\", 'themselves', 'wouldn', 'about', 'y', 'then', 'is', \"haven't\", 'as', 'while', 'our', 'same', 'those', 'once', 'we', 'your', 'but', 've', 'because', 'down', 'ain', 'having', 'has', 'myself', 'such', 'with', 'some', 'theirs', 'don', 'any', 'the', 'few', 'yours', 'until', 'both', 'from', 'can', 'at', 'haven', 'this', \"you've\", 'below', \"aren't\", 'ourselves', 'be', 'only', 'just', \"that'll\", 'of', 'yourself', 'i', 'and', 'on', \"isn't\", 'them', \"didn't\", \"hasn't\", 're', 'couldn', 'against', 'off', 'during'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cd4702fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'All', 'work', 'play', 'makes', 'Jack', 'dull', 'boy', \"''\", 'proverb', '.', 'It', 'means', 'without', 'time', 'work', ',', 'person', 'becomes', 'bored', 'boring', '.']\n"
     ]
    }
   ],
   "source": [
    "input_text = '\"All work and no play makes Jack a dull boy\" is a proverb. It means that without time off from work, a person becomes both bored and boring.'\n",
    "\n",
    "tokens = word_tokenize(input_text)\n",
    "\n",
    "print([i for i in tokens if i not in stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb8a03c",
   "metadata": {},
   "source": [
    "<h3>Stemming</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0014882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ca5a1736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "``\n",
      "all\n",
      "work\n",
      "and\n",
      "no\n",
      "play\n",
      "make\n",
      "jack\n",
      "a\n",
      "dull\n",
      "boy\n",
      "''\n",
      "is\n",
      "a\n",
      "proverb\n",
      ".\n",
      "it\n",
      "mean\n",
      "that\n",
      "without\n",
      "time\n",
      "off\n",
      "from\n",
      "work\n",
      ",\n",
      "a\n",
      "person\n",
      "becom\n",
      "both\n",
      "bore\n",
      "and\n",
      "bore\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for i in tokens :\n",
    "    print(stemmer.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1aad7039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have\n",
      "generous\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "stemmer2 = SnowballStemmer('english', ignore_stopwords = True)\n",
    "\n",
    "\n",
    "print(stemmer.stem('having'))\n",
    "print(stemmer.stem('generously'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "989807aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous\n",
      "gener\n"
     ]
    }
   ],
   "source": [
    "print(SnowballStemmer('english').stem('generously'))\n",
    "print(SnowballStemmer('porter').stem('generously'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bc0180",
   "metadata": {},
   "source": [
    "<h3>Lemmatisation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "301ffd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/aman.satyawali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fd9f8601",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = nltk.wordnet.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9991c7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meeting\n",
      "meet\n"
     ]
    }
   ],
   "source": [
    "print(lemma.lemmatize('meeting'))\n",
    "print(lemma.lemmatize('meeting', pos = 'v'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
